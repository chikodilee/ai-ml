# -*- coding: utf-8 -*-
"""Binary Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FUjOdyilqyxvJ1DmO8GnPOnAsarb34Ck
"""

#!pip install pandas
#!pip install seaborn

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#Get and read data
rawData = pd.read_csv('/DataA1.csv')
rawData.head()

#Describe each feature in the dataset
#rawData.describe()

# Classifier Parameter settings - KNN
kVals = np.arange(1, 31, 2)
knn = KNeighborsClassifier(n_neighbors = 5)

# Classifier Parameter settings - SVM
param_grid = {
    'C': [0.1, 0.5, 1, 2, 5, 10, 20, 50],
    'gamma': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]
}
# SVM with rbf kernel
svm = SVC(kernel='rbf')

#Preprocess Data Using Z-score Normalization
# (value - mean)/ std

#copy data
normData = rawData.copy()
#Separate features and target
featureData = rawData.drop('Label', axis=1)
target = rawData['Label']

#for loop to normalize the features
for col in (featureData):
  featureData[col] = (featureData[col] - featureData[col].mean())/featureData[col].std()

# Test and Plot normalized data using Feature 1
sns.distplot(featureData['Feature1'])

#Second Method to Normalize Data for Verification
sc = StandardScaler()
sc.fit(normData[['Feature1']])
sc_data = sc.transform(normData[['Feature1']])
sc_data = sc_data.reshape(-1)
sns.distplot(sc_data)

# Randomly split dataset into 70% training and 30% test
xTrain, xTest, yTrain, yTest = train_test_split(featureData, target, test_size=0.3, random_state=42)

# KNN -----------------------------------------------------------------------------
# 5-fold cross validation for k's
cv_scores = []

for k in kVal:
  knn.n_neighbors = k
  scores = cross_val_score(knn, xTrain, yTrain, cv=5, scoring='accuracy')
  cv_scores.append(scores.mean())

# Best k value - KNN
kBest = kVal[np.argmax(cv_scores)]
kBestAccuracy = cv_scores[np.argmax(cv_scores)]

# Plotting Results - KNN
plt.figure()
plt.plot(kVal, cv_scores)
plt.xlabel('Parameter k')
plt.ylabel('Accuracy')
plt.title('KNN - Relationship between Parameter K vs Accuracy')
plt.show()

# Best K - KNN
print(f'KNN Classifier Best k: {kBest}')
print(f'KNN Accuracy Value of Best k: {kBestAccuracy:.4f}')


# SVM -----------------------------------------------------------------------------
# 5-fold cross validation for c and gamma
gSearch = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
gSearch.fit(xTrain, yTrain)

# Best Params
bestParams = gSearch.best_params_

# Report best params - SVM
print(f'SVM Best Parameters: {bestParams}')


# Classifying the test set ---------------------------------------------------------

testClassifiers = [
    KNeighborsClassifier(n_neighbors=kBest),
    SVC(C=bestParams['C'], gamma=bestParams['gamma'], kernel='rbf'),
    GaussianNB(),
    DecisionTreeClassifier()
]

# Performance Metrics
testPerfMetrics = [
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
]

# Run classification 20 times
results =[]

for _ in range(20):
  # Split data
  xTrainII, xTestII, yTrainII, yTestII = train_test_split(featureData, target, test_size=0.3, random_state=None)

  # Training each classifier and evaluating metrics
  for classify in testClassifiers:
    classify.fit(xTrainII, yTrainII)
    yPredict = classify.predict(xTestII)

    #Calculating Metrics
    for metrics in testPerfMetrics:
      score = metrics(yTestII, yPredict)
      results.append((classify.__class__.__name__, metrics.__name__, score))


# Getting the average and results for all metrics
dataResults = pd.DataFrame(results, columns=["Classifier", "Metric", "Score"])
totalResults = dataResults.groupby(["Classifier", "Metric"])
resultSummary = totalResults.agg(["mean", "std"]).unstack()

print(resultSummary)
